#!/usr/bin/env python3
"""
Bayut Sub-location Level Scraper

Enhanced version of bayut_ldjson_to_mongo.py that scrapes at the sub-location level
to avoid Bayut's 50k property limit per location.

This script:
1. Reads from bayut_sublocations_all_cities.csv (generated by discover_sublocations.py)
2. Scrapes each sub-location's listing pages: /dubai/dubai-marina/page-{}/
3. Extracts LD+JSON property data and saves to MongoDB
4. Handles dynamic page ranges per sub-location

Usage:
    python bayut_sublocation_scraper.py --csv bayut_sublocations_all_cities.csv --max-pages 10

Features:
- Sub-location level scraping to avoid 50k limit
- CSV-driven approach for reproducible runs
- Organized output: out/html/{city}/{sublocation}/
- MongoDB tracking with full location hierarchy
- Smart page range detection (stops at 404s)
"""

import argparse
import csv
import datetime as dt
import hashlib
import json
import os
import re
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, UpdateOne, ASCENDING
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ----------------------------- Configuration ------------------------------

UA_DEFAULT = (
    os.getenv("BAYUT_UA")
    or "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
       "(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
)

HEADERS = {
    "User-Agent": UA_DEFAULT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
}

COOKIE_STR = os.getenv("COOKIE_STR", "").strip()
TIMEOUT = int(os.getenv("TIMEOUT_SEC", "30"))

DETAIL_ID_RE = re.compile(r"details-(\\d+)\\.html")

# ----------------------------- HTTP Session ------------------------------

def get_session() -> requests.Session:
    """Create a session with retry logic"""
    s = requests.Session()
    retry = Retry(
        total=3,
        connect=3,
        backoff_factor=1,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "HEAD"]),
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s

# ----------------------------- Extraction (Same as original) ------------------------------

def load_json_lenient(txt: str) -> Any:
    """Parse JSON, tolerating trailing commas (best-effort)."""
    try:
        return json.loads(txt)
    except Exception:
        fixed = re.sub(r",(\\s*[}\\]])", r"\\1", txt)
        return json.loads(fixed)

def extract_single_ldjson(html: str) -> Any:
    """
    Extract the ItemList LD+JSON containing property listings.
    Bayut pages have multiple LD+JSON blocks - we need the one with @type containing ItemList
    that has itemListElement with mainEntity (property data).
    """
    soup = BeautifulSoup(html, "lxml")
    tags = soup.find_all("script", attrs={"type": "application/ld+json"})
    
    if not tags:
        raise RuntimeError("No <script type='application/ld+json'> found.")
    
    # Try to find the ItemList with property data
    for tag in tags:
        raw = (tag.string or tag.text or "").strip()
        if not raw:
            continue
        
        try:
            data = load_json_lenient(raw)
            
            # Check if this has ItemList type (can be string or in array)
            if isinstance(data, dict):
                dtype = data.get("@type")
                has_itemlist = False
                
                # Check if @type contains ItemList
                if dtype == "ItemList":
                    has_itemlist = True
                elif isinstance(dtype, list) and "ItemList" in dtype:
                    has_itemlist = True
                
                # If it has ItemList type and itemListElement with mainEntity
                if has_itemlist and "itemListElement" in data:
                    items = data.get("itemListElement", [])
                    if items and len(items) > 0:
                        # Check if first item has mainEntity (property data)
                        first_item = items[0] if isinstance(items, list) else None
                        if first_item and "mainEntity" in first_item:
                            return data
        except Exception:
            # Skip malformed JSON blocks
            continue
    
    # Fallback: if no ItemList with properties found, try to find any ItemList
    for tag in tags:
        raw = (tag.string or tag.text or "").strip()
        if not raw:
            continue
        
        try:
            data = load_json_lenient(raw)
            if isinstance(data, dict):
                dtype = data.get("@type")
                if dtype == "ItemList" or (isinstance(dtype, list) and "ItemList" in dtype):
                    return data
        except Exception:
            continue
    
    # Last resort: return the first valid JSON found
    for tag in tags:
        raw = (tag.string or tag.text or "").strip()
        if raw:
            try:
                return load_json_lenient(raw)
            except Exception:
                continue
    
    raise RuntimeError("No valid LD+JSON found in any script tags.")

def property_id_from_url(u: Optional[str]) -> Optional[str]:
    if not u:
        return None
    m = DETAIL_ID_RE.search(u)
    if m:
        return m.group(1)
    return None

def doc_from_item(item: Dict[str, Any], page_num: int, fetched_at: str, location_info: Dict[str, str]) -> Dict[str, Any]:
    """
    Convert one itemListElement entry into a Mongo-ready document.
    Enhanced with full location hierarchy tracking.
    """
    pos = item.get("position")
    main = item.get("mainEntity") if isinstance(item, dict) else None
    url = None
    price = None
    if isinstance(main, dict):
        url = main.get("url") or None
        # Extract price from offers
        offers = main.get("offers", [])
        if offers and isinstance(offers[0], dict):
            price_spec = offers[0].get("priceSpecification", {})
            price = price_spec.get("price")
    pid = property_id_from_url(url)

    # last-resort key if no property_id is available
    if not pid:
        base = (url or json.dumps(main, ensure_ascii=False))[:256]
        pid = "hash_" + hashlib.md5(base.encode("utf-8", errors="ignore")).hexdigest()

    return {
        "_upsert_key": pid,
        "property_id": pid,
        "position": pos,
        "details_url": url,
        "page_number": page_num,
        "city": location_info["city"],
        "sublocation": location_info["sublocation"], 
        "location_url": location_info["url"],
        "estimated_listings": location_info["listings"],
        "fetched_at": fetched_at,
        "raw_item": item,
        "price": price,
    }

# ----------------------------- MongoDB Operations ------------------------------

def ensure_indexes(coll):
    try:
        coll.create_index([("property_id", ASCENDING)], unique=True, background=True)
        coll.create_index([("city", ASCENDING)], background=True)
        coll.create_index([("sublocation", ASCENDING)], background=True)
    except Exception as e:
        print(f"‚ö†Ô∏è  Index creation warning: {e}")

def bulk_upsert_items(coll, docs: List[Dict[str, Any]]):
    if not docs:
        return
    
    ops = []
    for d in docs:
        # Create appearance record with full location info
        appearance = {
            "page_number": d["page_number"],
            "position": d["position"],
            "city": d["city"],
            "sublocation": d["sublocation"],
            "price": d.get("price"),
            "scraped_at": d["fetched_at"]
        }
        
        # Use UpdateOne to track appearances with location hierarchy
        ops.append(
            UpdateOne(
                {"property_id": d["_upsert_key"]},
                {
                    "$set": {
                        "property_id": d["property_id"],
                        "details_url": d["details_url"],
                        "last_raw_item": d["raw_item"],
                        "last_seen": d["fetched_at"],
                        "last_page": d["page_number"],
                        "last_position": d["position"],
                        "last_city": d["city"],
                        "last_sublocation": d["sublocation"],
                        "current_price": d.get("price"),
                    },
                    "$addToSet": {
                        "cities_seen": d["city"],
                        "sublocations_seen": d["sublocation"]
                    },
                    "$push": {
                        "appearances": {
                            "$each": [appearance],
                            "$slice": -100  # Keep last 100 appearances
                        }
                    },
                    "$setOnInsert": {
                        "first_seen": d["fetched_at"],
                        "first_page": d["page_number"],
                        "first_city": d["city"],
                        "first_sublocation": d["sublocation"],
                        "detail_scraped": False,
                        "created_at": d["fetched_at"]
                    }
                },
                upsert=True
            )
        )
    
    res = coll.bulk_write(ops, ordered=False, bypass_document_validation=True)
    return res

# ----------------------------- File I/O ------------------------------

def save_html(html_dir: Path, page_num: int, content: str):
    html_dir.mkdir(parents=True, exist_ok=True)
    fp = html_dir / f"page_{page_num}.html"
    fp.write_text(content, encoding="utf-8", errors="ignore")

def save_json(json_dir: Path, page_num: int, data: Any):
    json_dir.mkdir(parents=True, exist_ok=True)
    fp = json_dir / f"page_{page_num}.json"
    with fp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_sublocations_csv(csv_path: str) -> List[Dict[str, str]]:
    """Load sublocation data from CSV"""
    sublocations = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            sublocations.append(row)
    return sublocations

# ----------------------------- Main Process ------------------------------

def scrape_sublocation(sess: requests.Session, location_info: Dict[str, str], args, coll) -> int:
    """Scrape a single sub-location and return number of items scraped"""
    city = location_info["city"].lower().replace(" ", "-")
    sublocation = location_info["sublocation"]
    
    # Create output directories
    html_dir = Path(args.out_dir) / "html" / city / sublocation
    json_dir = Path(args.out_dir) / "json" / city / sublocation
    html_dir.mkdir(parents=True, exist_ok=True)
    json_dir.mkdir(parents=True, exist_ok=True)
    
    # Extract city and sublocation slugs from URL
    # URL format: https://www.bayut.com/for-sale/property/dubai/dubai-marina/
    url_parts = location_info["url"].rstrip("/").split("/")
    city_slug = url_parts[-2]  # 'dubai'
    sublocation_slug = url_parts[-1]  # 'dubai-marina'
    
    base_url = f"https://www.bayut.com/for-sale/property/{city_slug}/{sublocation_slug}/page-{{}}/?sort=date_desc"
    
    print(f"\\nüè¢ === {city} > {sublocation} ({location_info['listings']} listings) ===")
    
    headers = dict(HEADERS)
    if COOKIE_STR:
        headers["cookie"] = COOKIE_STR
    
    total_items = 0
    consecutive_empty_pages = 0
    
    for page in range(1, args.max_pages + 1):
        if consecutive_empty_pages >= 5:
            print(f"  ‚èπÔ∏è  Stopping after {consecutive_empty_pages} consecutive pages with no results")
            break
            
        url = base_url.format(page)
        print(f"\\n  üìÑ Page {page}: {url}")
        
        try:
            r = sess.get(url, headers=headers, timeout=TIMEOUT)
            
            print(f"    üìä HTTP {r.status_code}, {len(r.text)} bytes")
            
            if r.status_code == 404:
                consecutive_empty_pages += 1
                print(f"    ‚ÑπÔ∏è  Page not found (404) - likely end of listings")
                continue
            elif r.status_code != 200:
                consecutive_empty_pages += 1  # Count non-200 as empty too
                print(f"    ‚ùå HTTP {r.status_code}")
                if r.status_code == 429:
                    print(f"    ‚è±Ô∏è  Rate limited - sleeping extra...")
                    time.sleep(10)
                continue
            html = r.text
            
            # Check for blocking indicators
            if len(html) < 1000:
                print(f"    ‚ö†Ô∏è  Small response: {len(html)} bytes")
            if "captcha" in html.lower() or "cloudflare" in html.lower():
                print(f"    ‚ö†Ô∏è  Possible blocking detected!")
            
            # Save files
            save_html(html_dir, page, html)
            
            # Extract and save LD+JSON
            ldjson = extract_single_ldjson(html)
            save_json(json_dir, page, ldjson)
            
            # Process items
            fetched_at = dt.datetime.utcnow().isoformat() + "Z"
            items = []
            
            if isinstance(ldjson, dict):
                elts = ldjson.get("itemListElement", [])
                for el in elts:
                    try:
                        doc = doc_from_item(el, page, fetched_at, location_info)
                        items.append(doc)
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è  Item parse error: {e}")
            
            if not items:
                consecutive_empty_pages += 1
                print(f"    ‚ÑπÔ∏è  No items found on page {page}")
                continue
            
            consecutive_empty_pages = 0  # Reset on successful extraction
            
            # Save to MongoDB
            res = bulk_upsert_items(coll, items)
            n_upserted = res.upserted_count or 0
            n_modified = res.modified_count or 0
            n_matched = res.matched_count or 0
            
            total_items += len(items)
            print(f"    ‚úÖ {len(items)} items (upserted={n_upserted}, matched={n_matched}, modified={n_modified})")
            
            # Rate limiting
            if page < args.max_pages:
                time.sleep(args.delay)
        
        except Exception as e:
            consecutive_empty_pages += 1  # Count exceptions as empty too
            print(f"    ‚ùå Error on page {page}: {e}")
            continue
    
    print(f"\\nüìä {city} > {sublocation}: {total_items} total items scraped")
    return total_items

def main(args):
    # Load sublocations
    print(f"üìÇ Loading sublocations from: {args.csv}")
    sublocations = load_sublocations_csv(args.csv)
    print(f"üåç Found {len(sublocations)} sub-locations to scrape")
    
    # Filter by city if specified
    if args.cities:
        target_cities = [city.strip().title() for city in args.cities.split(',')]
        sublocations = [loc for loc in sublocations if loc["city"] in target_cities]
        print(f"üéØ Filtered to {len(sublocations)} sub-locations in: {', '.join(target_cities)}")
    
    # Filter by minimum listings if specified
    if args.min_listings:
        sublocations = [
            loc for loc in sublocations 
            if loc["listings"] and int(loc["listings"].replace(",", "")) >= args.min_listings
        ]
        print(f"üìä Filtered to {len(sublocations)} sub-locations with >= {args.min_listings} listings")
    
    if not sublocations:
        print("‚ùå No sub-locations to process!")
        return
    
    # MongoDB connection
    client = MongoClient(args.mongo_uri, serverSelectionTimeoutMS=5000)
    client.admin.command("ping")
    db = client[args.db]
    coll = db[args.collection]
    ensure_indexes(coll)
    print(f"üóÑÔ∏è  MongoDB: {args.mongo_uri} ‚Üí {args.db}.{args.collection}")
    
    # HTTP session
    sess = get_session()
    
    # Process each sub-location
    total_items = 0
    processed = 0
    
    for i, location_info in enumerate(sublocations, 1):
        if args.start_from and i < args.start_from:
            continue
        if args.limit and processed >= args.limit:
            break
            
        print(f"\\n[{i}/{len(sublocations)}] Processing sub-location...")
        items_scraped = scrape_sublocation(sess, location_info, args, coll)
        total_items += items_scraped
        processed += 1
        
        # Delay between sub-locations
        if processed < (args.limit or len(sublocations)):
            print(f"\\n‚è±Ô∏è  Inter-location delay: {args.delay}s")
            time.sleep(args.delay)
    
    print(f"\\nüéâ Scraping complete!")
    print(f"üìä Sub-locations processed: {processed}")
    print(f"üìä Total items scraped: {total_items}")
    print(f"üìÇ Output directory: {Path(args.out_dir).resolve()}")

def parse_args():
    parser = argparse.ArgumentParser(
        description="Bayut Sub-location Level Scraper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Scrape all sub-locations (up to 10 pages each):
  python bayut_sublocation_scraper.py --csv bayut_sublocations_all_cities.csv --max-pages 10
  
  # Scrape only Dubai sub-locations:
  python bayut_sublocation_scraper.py --csv bayut_sublocations_all_cities.csv --cities "Dubai" --max-pages 5
  
  # Scrape high-volume sub-locations only:
  python bayut_sublocation_scraper.py --csv bayut_sublocations_all_cities.csv --min-listings 1000 --max-pages 20
  
  # Resume from sub-location #50:
  python bayut_sublocation_scraper.py --csv bayut_sublocations_all_cities.csv --start-from 50
        """
    )
    
    parser.add_argument("--csv", required=True, help="CSV file with sub-location data")
    parser.add_argument("--cities", help="Comma-separated cities to scrape (e.g., 'Dubai,Abu Dhabi')")
    parser.add_argument("--min-listings", type=int, help="Only scrape sub-locations with >= this many listings")
    parser.add_argument("--max-pages", type=int, default=10, help="Max pages to scrape per sub-location")
    parser.add_argument("--start-from", type=int, help="Resume from sub-location number")
    parser.add_argument("--limit", type=int, help="Limit number of sub-locations to process")
    parser.add_argument("--delay", type=float, default=2.0, help="Delay between requests (seconds)")
    
    parser.add_argument("--out-dir", default="out", help="Output directory")
    parser.add_argument("--mongo-uri", default="mongodb://localhost:27017", help="MongoDB URI")
    parser.add_argument("--db", default="bayut_production", help="MongoDB database")
    parser.add_argument("--collection", default="properties", help="MongoDB collection")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    try:
        main(args)
    except KeyboardInterrupt:
        print("\\nüõë Interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\\n‚ùå Fatal error: {e}")
        sys.exit(1)